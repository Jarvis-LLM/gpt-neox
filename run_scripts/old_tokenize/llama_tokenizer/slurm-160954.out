fatal: Not a valid object name HEAD
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
Vocab size: 32000
Output prefix: /fs/fast/u2020000280/data/github_llama_tokenized/train
/home/u2021000178/.conda/envs/llm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1713: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.
  warnings.warn(
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
0it [00:00, ?it/s] > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
Traceback (most recent call last):
  File "/home/share/gsai_joint_project/gpt-neox-2.0/tools/preprocess_data.py", line 244, in <module>
    main()
  File "/home/share/gsai_joint_project/gpt-neox-2.0/tools/preprocess_data.py", line 214, in main
    for i, (doc, bytes_processed) in enumerate(encoded_docs, start=1):
  File "/home/u2021000178/.conda/envs/llm/lib/python3.8/multiprocessing/pool.py", line 420, in <genexpr>
    return (item for chunk in result for item in chunk)
  File "/home/u2021000178/.conda/envs/llm/lib/python3.8/multiprocessing/pool.py", line 868, in next
    raise value
FileNotFoundError: /fs/fast/u2020000280/data/github/github-train.jsonl not found
0it [00:00, ?it/s]fatal: Not a valid object name HEAD
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
Vocab size: 32000
Output prefix: /fs/fast/u2020000280/data/github_llama_tokenized/val
/home/u2021000178/.conda/envs/llm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1713: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.
  warnings.warn(
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> building LlamaTokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
0it [00:00, ?it/s] > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
Traceback (most recent call last):
  File "/home/share/gsai_joint_project/gpt-neox-2.0/tools/preprocess_data.py", line 244, in <module>
    main()
  File "/home/share/gsai_joint_project/gpt-neox-2.0/tools/preprocess_data.py", line 214, in main
    for i, (doc, bytes_processed) in enumerate(encoded_docs, start=1):
  File "/home/u2021000178/.conda/envs/llm/lib/python3.8/multiprocessing/pool.py", line 420, in <genexpr>
    return (item for chunk in result for item in chunk)
  File "/home/u2021000178/.conda/envs/llm/lib/python3.8/multiprocessing/pool.py", line 868, in next
    raise value
FileNotFoundError: /fs/fast/u2020000280/data/github/github-val.jsonl not found
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
0it [00:00, ?it/s]