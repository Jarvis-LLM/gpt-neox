# Suggested data paths when using GPT-NeoX locally
{
  # "data-path": "./data/pile_00_text_document",

  # or for weighted datasets:
  # sample nums: [13704383051, 60761429, 441673231, 79901568160, 13242918819, 25936489268, 5180327559, 14382492596, 5565088327, 1606159768, 83138721815]
  "train-data-paths": [
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/zhihu/train_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/csl/train_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/wiki_wentong/train_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/cicg/train_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/cbook/train_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/pubmed_central/train_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/pubmed_abstract/train_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/freelaw/train_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/uspto/train_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/europarl/train_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/github_ours/train_text_document",
  ],
  # sample nums: [13803879, 59044, 415941, 79673535, 12346838, 379471665]
  "valid-data-paths": [
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/zhihu/val_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/csl/val_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/wiki_wentong/val_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/cicg/val_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/cbook/val_text_document",
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/pile_val_test/val_text_document", 
  ],
  "test-data-paths": [
    "/fs/fast/share/jarvis/tokenized_data/jarvis_v1/pile_val_test/test_text_document",
  ],
  # "train-data-weights": [1.0, 1.0, 1.0, 1.0, 1.0, 0.831, 0.831, 0.831, 0.831, 0.831, 0.12],
  # "valid-data-weights": [1.0, 1.0, 1.0, 1.0, 1.0, 0.06],
  "train-data-weights": [2.27, 2.27, 2.27, 2.27, 2.27, 4.61, 4.61, 4.61, 4.61, 4.61, 2.92],
  "valid-data-weights": [4.57, 4.57, 4.57, 4.57, 4.57, 1.28],
  #   "train-data-weights": [1.0, 1.0, 1.0, 1.0, 1.0],
  # "valid-data-weights": [1.0],
  "test-data-weights": [1.0],

  "vocab-file": "/fs/fast/share/jarvis/tokenizer/jarvis_tokenizer_v1/tokenizer.model",
  "tokenizer_type": "LlamaTokenizer",


  "save": "/home/u2021000178/share/gsai_joint_project/llama_train/gpt-neox-main/checkpoints_65B_v1",
  "load": "/fs/fast/share/jarvis/checkpoints/65B/v1",

  # ### open the below two arguements when training from the original llama-65B model
  # "finetune": True,
  # "initialize_llama_cn_with_llama_word_embeddings": True,

  "checkpoint_validation_with_forward_pass": False,

  # #new added parameters for training llama-cn
  # "only_train_new_chinese": True,
  "log_dir": "65B_v1_logs",
  "use_wandb": True,
  "wandb_host": "https://api.wandb.ai",
  "wandb_project": "65B_v1",
  "wandb_team": "jarvis_llm"
}
